import time
from cs336_basics.tokenizer import Tokenizer

def load_sample_docs(filepath, num_docs=10):
    """è¯»å–å‰ N ä¸ªæ–‡æ¡£"""
    with open(filepath, "r", encoding="utf-8") as f:
        # è¯»å–è¶³å¤Ÿå¤šçš„å­—ç¬¦ä»¥ç¡®ä¿èƒ½æå–å‡º 10 ä¸ªæ–‡æ¡£
        content = f.read(100000) 
    docs = [d for d in content.split("<|endoftext|>") if d.strip()]
    return docs[:num_docs]

def calc_compression(tokenizer, docs, tokenizer_name, data_name):
    """è®¡ç®—å‹ç¼©æ¯”: åŸå§‹å­—èŠ‚æ•° / Token æ•°é‡"""
    total_bytes = 0
    total_tokens = 0
    for doc in docs:
        total_bytes += len(doc.encode("utf-8"))
        total_tokens += len(tokenizer.encode(doc))
    
    ratio = total_bytes / total_tokens if total_tokens > 0 else 0
    print(f"[{tokenizer_name} Tokenizer] ç¼–ç  [{data_name} æ•°æ®] -> å‹ç¼©æ¯”: {ratio:.2f} bytes/token")
    return ratio

def main():
    print("â³ æ­£åœ¨åŠ è½½ä¸¤ä¸ª Tokenizer...\n")
    # åŠ è½½ TinyStories (10K)
    ts_tokenizer = Tokenizer.from_files(
        "../tinystories_vocab.json", "../tinystories_merges.txt", special_tokens=["<|endoftext|>"]
    )
    # åŠ è½½ OWT (32K) - è¯·ç¡®ä¿æ–‡ä»¶åå’Œä½ ä¿å­˜çš„ä¸€è‡´
    owt_tokenizer = Tokenizer.from_files(
        "../owt_valid_vocab.json", "../owt_valid_merges.txt", special_tokens=["<|endoftext|>"]
    )

    print("ğŸ“– æ­£åœ¨åŠ è½½æµ‹è¯•æ ·æœ¬...")
    # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„
    ts_docs = load_sample_docs("../data/TinyStoriesV2-GPT4-valid.txt", 10) 
    owt_docs = load_sample_docs("../data/owt_valid.txt", 10)

    print("\nğŸ“Š --- Deliverable (a): å„è‡ªé¢†åŸŸçš„å‹ç¼©æ¯” ---")
    calc_compression(ts_tokenizer, ts_docs, "TinyStories", "TinyStories")
    calc_compression(owt_tokenizer, owt_docs, "OpenWebText", "OpenWebText")

    print("\nğŸ“‰ --- Deliverable (b): è·¨é¢†åŸŸåˆ†è¯æµ‹è¯• ---")
    calc_compression(ts_tokenizer, owt_docs, "TinyStories", "OpenWebText")

    print("\nğŸš€ --- Deliverable (c): ååé‡ä¸ The Pile è€—æ—¶ä¼°ç®— ---")
    # ä½¿ç”¨ä¸€æ®µè¾ƒé•¿çš„æ–‡æœ¬æµ‹è¯• OWT Tokenizer çš„çº¯ç¼–ç é€Ÿåº¦
    test_text = "".join(owt_docs) * 5
    text_bytes = len(test_text.encode("utf-8"))
    
    start_time = time.time()
    _ = owt_tokenizer.encode(test_text)
    elapsed = time.time() - start_time
    
    throughput = text_bytes / elapsed
    print(f"åˆ†è¯ååé‡: {throughput:,.2f} bytes/second")
    
    # è®¡ç®— 825GB éœ€è¦çš„æ—¶é—´
    pile_bytes = 825 * 1024**3
    pile_hours = (pile_bytes / throughput) / 3600
    print(f"ä¼°ç®—å¤„ç† The Pile (825GB) è€—æ—¶: {pile_hours:,.2f} å°æ—¶")

if __name__ == "__main__":
    main()